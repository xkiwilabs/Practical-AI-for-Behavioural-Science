<p><strong>Class time:</strong> Mondays, 3:00–5:00pm</p>
<p><strong>Teaching dates:</strong> 23 Feb 2026 – 01 Jun 2026</p>
<p><strong>Mid-semester break (no classes):</strong> 06 Apr 2026 – 19 Apr 2026</p>

<hr>

<h2>Course Structure</h2>
<p>The course alternates between <strong>Lecture weeks</strong> and <strong>Challenge Lab weeks</strong>.</p>

<h3>Lecture Weeks</h3>
<p>Lectures introduce concepts, methods, and critical evaluation frameworks. The first 20 minutes of each lecture (from Week 3 onwards) feature <strong>group solution presentations</strong> from the previous week's challenge — each group presents their solution on a single PowerPoint slide. These presentations are not graded, but the goal is to develop the best solution and learn from how other groups approached the problem.</p>

<h3>Challenge-Lab Weeks</h3>
<p>Challenge-Lab weeks are hands-on sessions where you work in small groups (3–4 students) to solve real analysis problems using LLM-assisted coding.</p>
<p><strong>Lab structure:</strong></p>
<ul>
    <li><strong>First 30 minutes:</strong> Student paper presentations (Weeks 4, 6, 8, 10)</li>
    <li><strong>Next 1.5 hours:</strong> Group challenge work — practice the LLM problem-solving loop</li>
    <li><strong>Following week:</strong> Present your solution (1 slide, ~2 min per group) at the start of the next lecture</li>
</ul>
<p><strong>Homework:</strong> There are no required readings for this course. Your out-of-class work is completing your group's challenge solution and preparing your 1-slide presentation. If not finished during the lab, expect to spend 1–2 hours with your group to finalise your solution.</p>
<p><em>Note: Topics and timing may be adjusted based on class progress and needs.</em></p>

<hr>

<h2>Weekly Schedule</h2>

<table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; width: 100%;">
    <thead>
        <tr style="background-color: #c41e3a; color: white;">
            <th style="width: 8%;">Week</th>
            <th style="width: 14%;">Date</th>
            <th style="width: 10%;">Type</th>
            <th style="width: 68%;">Topic &amp; Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><strong>1</strong></td>
            <td>Mon 23 Feb</td>
            <td>Lecture</td>
            <td>
                <strong>Why ML and AI Belongs in Psychological Science</strong><br>
                Introduce the course goals, core ML concepts, and how AI tools change psychological research. Clarify prediction versus explanation, generalisation, and common threats like confounds and data leakage. Establish the "LLM problem-solving loop" students will use all semester.<br>
            </td>
        </tr>
        <tr style="background-color: #f9f9f9;">
            <td><strong>2</strong></td>
            <td>Mon 02 Mar</td>
            <td>Challenge-Lab</td>
            <td>
                <strong>Coding Lab 1: Setup, Plotting, and the LLM Problem-Solving Loop</strong><br>
                Lab setup: Python/Jupyter, core libraries, and a reproducible workflow. Practice prompting an LLM for code generation, verifying outputs with plots and shape checks.<br>
                <em>Group Challenge: Reproduce a Figure</em>
            </td>
        </tr>
        <tr>
            <td><strong>3</strong></td>
            <td>Mon 09 Mar</td>
            <td>Lecture</td>
            <td>
                <strong>Models, Not Magic: Generalisation, Overfitting, and How ML Misleads</strong><br>
                Foundations of supervised learning: targets, features, preprocessing, baselines, and evaluation. Cover train/test splits, cross-validation, bias–variance trade-offs, and how overfitting appears in behavioural datasets.<br>
                <em>Week 2 Challenge-Lab solutions - presentations</em>
            </td>
        </tr>
        <tr style="background-color: #f9f9f9;">
            <td><strong>4</strong></td>
            <td>Mon 16 Mar</td>
            <td>Challenge-Lab</td>
            <td>
                <strong>Coding Lab 2: Regression Pipeline, Cross-Validation, and Interpretation</strong><br>
                Build end-to-end pipelines for linear and regularised models (Ridge/Lasso). Use cross-validation, learning curves, and residual diagnostics to detect misfit.<br>
                <em>Group Challenge: Predict &amp; Explain</em><br>
                <em>Student paper presentations</em>
            </td>
        </tr>
        <tr>
            <td><strong>5</strong></td>
            <td>Mon 23 Mar</td>
            <td>Lecture</td>
            <td>
                <strong>Prediction with Accountability: Classification, Uncertainty, and Evaluation</strong><br>
                Classification for psychological prediction: logistic regression, decision thresholds, class imbalance. Compare metrics (accuracy, F1, ROC-AUC, PR-AUC) and discuss ethical risks of categorising people.<br>
                <em>Week 4 Challenge-Lab solutions - presentations</em>
            </td>
        </tr>
        <tr style="background-color: #f9f9f9;">
            <td><strong>6</strong></td>
            <td>Mon 30 Mar</td>
            <td>Challenge-Lab</td>
            <td>
                <strong>Coding Lab 3: Trees, Ensembles, Feature Importance, and Error Analysis</strong><br>
                Implement decision trees and random forests. Focus on tuning, cross-validation, and diagnosing failure modes via error analysis.<br>
                <em>Group Challenge: Build a Defensible Classifier</em><br>
                <em>Student paper presentations</em>
            </td>
        </tr>
        <tr style="background-color: #e8e8e8;">
            <td colspan="4" style="text-align: center;"><strong>Mid-Semester Break — 06 Apr 2026 – 19 Apr 2026</strong><br><strong style="color: #c41e3a;">Written assignment due: Sunday 19 Apr, 11:55pm</strong></td>
        </tr>
        <tr>
            <td><strong>7</strong></td>
            <td>Mon 20 Apr</td>
            <td>Lecture</td>
            <td>
                <strong>Discovering Structure: Clustering and Dimensionality Reduction Without Self-Deception</strong><br>
                Unsupervised learning: clustering (k-means, hierarchical, DBSCAN) and dimensionality reduction (PCA vs. UMAP/t-SNE). Emphasise stability and the danger of reifying "types."<br>
                <em>Week 6 Challenge-Lab solutions - presentations</em>
            </td>
        </tr>
        <tr style="background-color: #f9f9f9;">
            <td><strong>8</strong></td>
            <td>Mon 27 Apr</td>
            <td>Challenge-Lab</td>
            <td>
                <strong>Coding Lab 4: PCA/UMAP, Clustering, and Stability Checks</strong><br>
                Apply dimensionality reduction and clustering. Evaluate solutions using silhouette scores, resampling stability, and sanity checks.<br>
                <em>Group Challenge: Find Structure, Don't Fabricate It</em><br>
                <em>Student paper presentations</em>
            </td>
        </tr>
        <tr>
            <td><strong>9</strong></td>
            <td>Mon 04 May</td>
            <td>Lecture</td>
            <td>
                <strong>Learning Representations: Neural Networks as Psychological Tools (and Limits)</strong><br>
                Neural networks as function approximators: layers, losses, optimisation, regularisation. When deep learning helps and when simpler models are better.<br>
                <em>Week 8 Challenge-Lab solutions - presentations</em>
            </td>
        </tr>
        <tr style="background-color: #f9f9f9;">
            <td><strong>10</strong></td>
            <td>Mon 11 May</td>
            <td>Challenge-Lab</td>
            <td>
                <strong>Coding Lab 5: Simple Neural Network, Training Diagnostics, and Baseline Comparison</strong><br>
                Build and train a small MLP in PyTorch. Compare to classical baselines and troubleshoot common problems.<br>
                <em>Group Challenge: Beat the Baseline</em><br>
                <em>Student paper presentations</em>
            </td>
        </tr>
        <tr>
            <td><strong>11</strong></td>
            <td>Mon 18 May</td>
            <td>Lecture & Challenge-Lab</td>
            <td>
                <strong>Meaning in Vectors: Embeddings and LLMs for Research Workflows</strong><br>
                Embeddings and LLMs as research infrastructure: semantic similarity, text clustering, retrieval, and assisted coding. How LLMs work under the hood.<br>
                <em>Group Challenge: The Power of LLMs for Data Coding, Classification and Analysis</em><br>
                <em>Week 10 Challenge-Lab solutions - presentations</em>
            </td>
        </tr>
        <tr style="background-color: #f9f9f9;">
            <td><strong>12</strong></td>
            <td>Mon 25 May</td>
            <td>Viva Review</td>
            <td>
                <strong>Viva Review: Consolidation, Q&amp;A, and Practice Defenses</strong><br>
                No new content. Practice defenses of written work and methods choices. Q&amp;A and exam guidance.
            </td>
        </tr>
        <tr>
            <td><strong>13</strong></td>
            <td>Mon 01 Jun</td>
            <td>Discussion</td>
            <td>
                <strong>What You Can Claim: Limits, Ethics, Reproducibility, and Reasoned Conclusions</strong><br>
                Wrap-up on the boundaries of ML and AI in psychological science. Reproducibility, bias, privacy, and responsible communication of uncertainty.<br><br>
                <strong>Course Reflection:</strong> Open discussion on the LLM-assisted learning format. What worked? What would you change? How has your approach to using AI tools evolved? Your feedback will shape future iterations of this course.<br>
                <em> Week 11 Challenge-Lab solutions - presentations</em>
            </td>
        </tr>
    </tbody>
</table>
